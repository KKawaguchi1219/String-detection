{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPiS/5nJZOZw6+Z1pkN9Hll",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KKawaguchi1219/string-detection/blob/main/string_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MpxZWg6-t2_-",
        "outputId": "215bd4c6-3a13-48af-8594-2ae87d2609c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# 検出したい語句が書かれたテキストファイル名を入力\n",
        "# ただし、このテキストファイルは１語句ごとに改行されているものとする\n",
        "path = 'wordlist.txt'\n",
        "with open(path, 'r') as f:\n",
        "  List = f\n",
        "  wordlist = [i.replace('\\n', '') for i in  List.readlines()]\n",
        "  wordlist = [i.replace('-', ' ') for i in  wordlist]\n",
        "\n",
        "# 英文が書かれたテキストファイルの名前を入力\n",
        "file_name = 'test.txt'\n",
        "string = Path(file_name).read_text().translate(str.maketrans({'.': None, ',': None, '!' : None, '?' : None, '\"' : None, '\\'' : None, '\\n' : ' '}))\n",
        "words = [i for i in string.split()]\n",
        "\n",
        "# nltkだと精度が細かすぎて文字数が増えるので、処理を挟む前に文字数をカウント\n",
        "print(words)\n",
        "print(\"{} 文字\".format(len(words)))\n",
        "\n",
        "string = string.translate(str.maketrans({'-' : ' '}))\n",
        "words = nltk.word_tokenize(string)\n",
        "pos = nltk.pos_tag(words)\n",
        "\n",
        "# pandasの練習もかねて\n",
        "df_pos = pd.DataFrame(pos, columns=['word', 'pos'])\n",
        "\n",
        "# 原型に変換...精度は不明\n",
        "lemmatizer =WordNetLemmatizer()\n",
        "verb = ['VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n",
        "adverb = ['RBR', 'RBS', 'RP']\n",
        "adj = ['JJR', 'JJS']\n",
        "noun = ['NNS']\n",
        "for i in range(len(words)):\n",
        "  if df_pos.at[i, 'pos'] in verb:\n",
        "    df_pos.at[i, 'word'] = lemmatizer.lemmatize(df_pos.at[i, 'word'], pos=\"v\")\n",
        "    df_pos.at[i, 'pos'] = 'VB'\n",
        "  elif df_pos.at[i, 'pos'] in adverb:\n",
        "    df_pos.at[i, 'word'] = lemmatizer.lemmatize(df_pos.at[i, 'word'], pos=\"r\")\n",
        "    df_pos.at[i, 'pos'] = 'RB'\n",
        "  elif df_pos.at[i, 'pos'] in adj:\n",
        "    df_pos.at[i, 'word'] = lemmatizer.lemmatize(df_pos.at[i, 'word'], pos=\"a\")\n",
        "    df_pos.at[i, 'pos'] = 'JJ'\n",
        "  elif df_pos.at[i, 'pos'] in noun:\n",
        "    df_pos.at[i, 'word'] = lemmatizer.lemmatize(df_pos.at[i, 'word'], pos=\"n\")\n",
        "    df_pos.at[i, 'pos'] = 'NN'\n",
        "\n",
        "# パターンマッチング　(ただし、原形に変換したので複数形の名詞を含む熟語はマッチしない)\n",
        "words = df_pos['word'].to_list()\n",
        "memo = set()\n",
        "for i in range(len(wordlist)):\n",
        "  # 検索するidiomの文字数\n",
        "  num_of_word = len(wordlist[i].split())\n",
        "\n",
        "  for j in range(len(words)):\n",
        "    idiom = [words[k] for k in range(j, j+num_of_word)  if k < len(words)]\n",
        "    # 検索するidiomの文字数以下は捨てる\n",
        "    if len(idiom) < num_of_word:\n",
        "      continue\n",
        "    # 分割された単語を半角スペースで繋げる\n",
        "    idiom = ' '.join(idiom)\n",
        "\n",
        "    if idiom in wordlist:\n",
        "      memo.add(idiom)\n",
        "\n",
        "print(\"使用語句：{}\".format(list(memo)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qUOX9kUKt_vD",
        "outputId": "f89397b9-db9e-43c5-ae9e-a1a2110b5f06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['stocks', 'and', 'shares', 'investment', 'investor', 'values', 'returned', 'standard', 'of', 'living', 'income', 'fossil-fuel', 'investor', 'greenhouse-gas', 'self-esteem']\n",
            "15 文字\n",
            "使用語句：['investor', 'value', 'fuel', 'greenhouse gas', 'return', 'income', 'fossil fuel', 'self esteem', 'standard of living', 'investment']\n"
          ]
        }
      ]
    }
  ]
}